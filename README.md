# DataVault ETL Project

## Overview

This project involves the extraction, transformation, and loading (ETL) of data from AWS S3 to a Vertica database. The ETL pipeline is orchestrated using Apache Airflow and containerized using Docker. SQL and Python are the primary languages for data manipulation and analysis.

## Technologies Used

* **AWS S3:**  Object storage for raw data.
* **Vertica:** Columnar database for high-performance analytics.
* **Apache Airflow:**  Workflow orchestration tool to manage the ETL pipeline.
* **Docker:** Containerization for streamlined development and deployment.
* **SQL:**  For querying and manipulating data within Vertica.
* **Python:**  For data transformation and pipeline logic.

## Analysis and Queries

- Staging Data Analysis: [Link to Staging Data Analysis](staging_data_analysis.md)
- DDS Data Analysis: [Link to DDS Data Analysis](dds_data_analysis.md)

